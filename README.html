<html><head>
    <meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=KAcgnVa0IBppVRWtHAiHYquCQmIIa9sm9nBQ4AChcoU');ul.lst-kix_ci6wapdmgw23-4{list-style-type:none}ul.lst-kix_ci6wapdmgw23-3{list-style-type:none}.lst-kix_ci6wapdmgw23-7>li:before{content:"\0025cb  "}.lst-kix_ci6wapdmgw23-8>li:before{content:"\0025a0  "}ul.lst-kix_ci6wapdmgw23-2{list-style-type:none}ul.lst-kix_ci6wapdmgw23-1{list-style-type:none}.lst-kix_iwlwm8lusqnh-5>li:before{content:"\0025a0  "}.lst-kix_iwlwm8lusqnh-7>li:before{content:"\0025cb  "}ul.lst-kix_ci6wapdmgw23-8{list-style-type:none}.lst-kix_ci6wapdmgw23-6>li:before{content:"\0025cf  "}ul.lst-kix_ci6wapdmgw23-7{list-style-type:none}ul.lst-kix_ci6wapdmgw23-6{list-style-type:none}ul.lst-kix_ci6wapdmgw23-5{list-style-type:none}.lst-kix_iwlwm8lusqnh-2>li:before{content:"\0025a0  "}.lst-kix_iwlwm8lusqnh-6>li:before{content:"\0025cf  "}.lst-kix_iwlwm8lusqnh-3>li:before{content:"\0025cf  "}.lst-kix_iwlwm8lusqnh-4>li:before{content:"\0025cb  "}ul.lst-kix_iwlwm8lusqnh-8{list-style-type:none}ul.lst-kix_iwlwm8lusqnh-7{list-style-type:none}ul.lst-kix_iwlwm8lusqnh-6{list-style-type:none}ul.lst-kix_iwlwm8lusqnh-5{list-style-type:none}ul.lst-kix_iwlwm8lusqnh-4{list-style-type:none}ul.lst-kix_iwlwm8lusqnh-3{list-style-type:none}ul.lst-kix_iwlwm8lusqnh-2{list-style-type:none}ul.lst-kix_iwlwm8lusqnh-1{list-style-type:none}ul.lst-kix_iwlwm8lusqnh-0{list-style-type:none}.lst-kix_iwlwm8lusqnh-1>li:before{content:"\0025cb  "}ul.lst-kix_ci6wapdmgw23-0{list-style-type:none}.lst-kix_iwlwm8lusqnh-0>li:before{content:"\0025cf  "}.lst-kix_ci6wapdmgw23-0>li:before{content:"\0025cf  "}.lst-kix_ci6wapdmgw23-1>li:before{content:"\0025cb  "}.lst-kix_ci6wapdmgw23-2>li:before{content:"\0025a0  "}.lst-kix_ci6wapdmgw23-4>li:before{content:"\0025cb  "}.lst-kix_ci6wapdmgw23-3>li:before{content:"\0025cf  "}.lst-kix_ci6wapdmgw23-5>li:before{content:"\0025a0  "}.lst-kix_iwlwm8lusqnh-8>li:before{content:"\0025a0  "}ol{margin:0;padding:0}table td,table th{padding:0}.c4{font-size:12pt;font-family:"Consolas";color:#006666}.c14{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c6{font-size:12pt;font-family:"Consolas";color:#000000}.c5{margin-left:72pt;padding-left:0pt}.c0{orphans:2;widows:2}.c13{margin-left:36pt;padding-left:0pt}.c10{color:#1155cc;text-decoration:underline}.c7{padding:0;margin:0}.c11{color:inherit;text-decoration:inherit}.c12{font-size:12pt}.c9{font-style:italic}.c2{page-break-after:avoid}.c1{height:12pt}.c8{font-weight:bold}.c3{text-align:left}.c15{text-align:center}.title{padding-top:0pt;color:#000000;font-size:21pt;padding-bottom:0pt;font-family:"Droid Sans";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:13pt;padding-bottom:10pt;font-family:"Trebuchet MS";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:12pt;font-family:"Droid Sans"}p{margin:0;color:#000000;font-size:12pt;font-family:"Droid Sans"}h1{padding-top:10pt;color:#000000;font-size:16pt;padding-bottom:0pt;font-family:"Droid Sans";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:10pt;color:#000000;font-weight:bold;font-size:13pt;padding-bottom:0pt;font-family:"Droid Sans";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:8pt;color:#666666;font-weight:bold;font-size:12pt;padding-bottom:0pt;font-family:"Trebuchet MS";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:8pt;color:#666666;text-decoration:underline;font-size:11pt;padding-bottom:0pt;font-family:"Trebuchet MS";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:8pt;color:#666666;font-size:11pt;padding-bottom:0pt;font-family:"Trebuchet MS";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:8pt;color:#666666;font-size:11pt;padding-bottom:0pt;font-family:"Trebuchet MS";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c14"><p class="c0 c2 title"><a id="h.yt9bx6fnkwpo"></a><span>CSI5149 Project</span></p><h2 class="c0 c2"><a id="h.ylh8448rpyw1"></a><span>Shaughn Finnerty, 6300433</span></h2><p class="c0 c1"><span></span></p><p class="c0"><span>In this project we use two algorithms, sum-product and max-product on a factor graphs with cycles to represent the specific (7,4) Hamming Code specified.</span></p><p class="c0 c1"><span></span></p><p class="c0"><span>The resulting factor graph of this error correction code looks as follows:</span></p><h1 class="c0 c2"><a id="h.t038cdg081j2"></a><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 563.00px; height: 243.00px;"><img alt="" src="images/image15.png" style="width: 563.00px; height: 243.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></h1><p class="c0 c1"><span></span></p><p class="c0"><span>The</span><span>&nbsp;Global Function is </span><img src="images/image00.png"><span>&nbsp;when the codeword </span><img src="images/image01.png"><span>is in the list of valid codewords and 0 otherwise. That is our global membership indicator function for the projects error code looks as follows:</span></p><p class="c0 c1"><span></span></p><p class="c0 c15"><img src="images/image02.png"></p><p class="c0 c1 c3"><span></span></p><p class="c0 c3"><span>With factors defined as follows:</span></p><p class="c0 c1 c3"><span></span></p><p class="c0 c3"><img src="images/image03.png"></p><p class="c0 c1 c3"><span></span></p><p class="c0 c3"><span>Where </span><img src="images/image04.png"><span>&nbsp;is the Dirac Delta function (1 when x=0, 0 everywhere else for x). Similar definitions for factors </span><img src="images/image05.png"><span>&nbsp;and </span><img src="images/image06.png"><span>&nbsp;are assumed.</span></p><p class="c0 c1"><span></span></p><p class="c0"><span>As a result, the following codewords are those which belong in this specific 16 word code satisfying the system of equations defined in this project:</span></p><p class="c0 c1"><span></span></p><p class="c0"><span class="c4">0000000</span></p><p class="c0"><span class="c4">0001111</span></p><p class="c0"><span class="c4">0010110</span></p><p class="c0"><span class="c4">0011001</span></p><p class="c0"><span class="c4">0100101</span></p><p class="c0"><span class="c4">0101010</span></p><p class="c0"><span class="c4">0110011</span></p><p class="c0"><span class="c4">0111100</span></p><p class="c0"><span class="c4">1000011</span></p><p class="c0"><span class="c4">1001100</span></p><p class="c0"><span class="c4">1010101</span></p><p class="c0"><span class="c4">1011010</span></p><p class="c0"><span class="c4">1100110</span></p><p class="c0"><span class="c4">1101001</span></p><p class="c0"><span class="c4">1110000</span></p><p class="c0"><span class="c4">1111111</span></p><p class="c0 c1"><span></span></p><p class="c0"><span>Using the the received (observed) codewords </span><img src="images/image07.png"><span>&nbsp;at the decoder, the graphical model for this then becomes.</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 566.00px; height: 443.00px;"><img alt="" src="images/image14.png" style="width: 566.00px; height: 443.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c1"><span></span></p><p class="c0"><span class="c8 c9">Important assumption/note</span><span>: Since the factor nodes </span><img src="images/image08.png"><span>have only degree one after being evaluated using function `Decoder.compute_prior_prob(x, z)` which returns the value at z of the CDF of the normal distribution with the variance of the channel (we assume that channel noise is known to the decoder) and are purely a univariate function of </span><img src="images/image09.png"><span>respectively. &nbsp; Essentially, we eliminate observed nodes by absorbing them (with their observed constant values) into the corresponding factors. </span><span class="c8">Since the factor node </span><img src="images/image10.png"><span class="c8">&nbsp;becomes a leaf node (degree one) after being evaluated then their message </span><img src="images/image11.png"><span class="c8">&nbsp;will never change and so we include this message in the variable nodes implicitly and use it when computing messages from variable nodes throughout the iterations of the algorithm.</span></p><p class="c0 c1"><span></span></p><h1 class="c0 c2"><a id="h.ta4xsumnumg3"></a><span>Implementation </span><span>Assumptions/Notes:</span></h1><p class="c0 c1"><span></span></p><p class="c0"><span class="c8">Some important assumptions about this implementation</span><span>:</span></p><ul class="c7 lst-kix_ci6wapdmgw23-0 start"><li class="c0 c13"><span>Each undirected edge is represented by two directed edges. This was decided to allow for better identification/separation of </span><span class="c8">messages</span><span>&nbsp;and their directions. Together they form the undirected edge as required by factor graphs.</span></li><li class="c0 c13"><span>Since we are operating on a factor graph with cycles, we initiate all messages from variable nodes to factor nodes with values 1 for every value in the message domain (e.g. 0 and 1 in this case).</span></li><li class="c0 c13"><span>In this cyclic graphical model, we use a flooding iterative schedule to pass messages. The algorithm </span><span class="c8">terminates</span><span>&nbsp;when messages are passed that allow a final belief to be computed that produces a decoded codeword that exists in the 16 possibles codewords OR if a max number of iterations has occurred (e.g. we have this max set to 20 iterations).</span></li><li class="c0 c13"><span>When computing bit error probability, we compare decoded codewords with the original codewords that are sent (assume the decoder knows of them). There is a boolean static Flag `USE_0_COMPUTATION` in `simulator.py` that can be changed to `True` to assume that the original codeword is (0,0,0,0,0,0,0) as suggested in the project details.</span></li></ul><p class="c0 c1"><span></span></p><h1 class="c0 c2"><a id="h.s5gvwtgfbo1"></a><span>Running Instructions &amp; Dependencies</span></h1><p class="c0 c1"><span></span></p><p class="c0"><span>To run this project, you will need the following</span></p><p class="c0 c1"><span></span></p><ul class="c7 lst-kix_iwlwm8lusqnh-0 start"><li class="c0 c13"><span class="c8">Python 2.7</span></li><li class="c0 c13"><span class="c8">numpy</span><span>&nbsp;for math operations (i.e. computing codewords from parity check matrix)</span></li></ul><ul class="c7 lst-kix_iwlwm8lusqnh-1 start"><li class="c0 c5"><span>`pip install numpy`</span></li><li class="c5 c0"><span>http://docs.scipy.org/doc/numpy-1.10.1/user/install.html</span></li></ul><ul class="c7 lst-kix_iwlwm8lusqnh-0"><li class="c0 c13"><span class="c8">matplotlib </span><span>for plotting variance vs. bit error probability (in logarithmic domain) </span></li></ul><ul class="c7 lst-kix_iwlwm8lusqnh-1 start"><li class="c5 c0"><span>`pip install matplotlib`</span></li><li class="c5 c0"><span class="c10"><a class="c11" href="https://www.google.com/url?q=http://matplotlib.org/users/installing.html&amp;sa=D&amp;ust=1456713334524000&amp;usg=AFQjCNFBTvWCQG3jntbCmRBD8H5EeYCGsw">http://matplotlib.org/users/installing.html</a></span></li></ul><p class="c0 c1"><span></span></p><p class="c0"><span>As seen the pip package manager for python is the best tool to install these dependencies. This package manager can be installed by following the instructions here @ https://pip.pypa.io/en/stable/installing/ (downloading a python file and executing it)</span></p><p class="c0 c1"><span></span></p><h2 class="c0 c2"><a id="h.b701qnz7qtj6"></a><span>Running Instructions</span></h2><p class="c0 c1"><span></span></p><p class="c0"><span>To run the project and generate a plot containing variance vs. bit error probability for both algorithms run the following command:</span></p><p class="c0 c1"><span></span></p><p class="c0"><span>`python run.py &lt;num_codewords&gt;`</span></p><p class="c0 c1"><span></span></p><p class="c0"><span>where `&lt;num_codewords&gt;` is the number of codewords to test at each variance level for each algorithm. In our results, we used 2000. With 2000 codewords, running both algorithms at 4 different variance levels each, this takes approximately 10 minutes on a MBP 2011 2.2 GHz Intel Core i7.</span></p><p class="c0 c1"><span></span></p><p class="c0"><span>This will show a plot at the end and also save the file in the </span><span class="c8">graphs </span><span>directory as an SVG with the epoch timestamp in the filename.</span></p><p class="c0 c1"><span></span></p><p class="c0"><span class="c8">Alternatively</span><span>, if you &nbsp;would like to get an individual graph for each algorithm, some more specific simulations can be run with the `-advanced` flag. &nbsp;This will decode 2000 codewords at the variance levels required for the project, followed by decoding 1000 codewords at variance levels `[0.1, 0.2,...,0.9,1.0]` for each algorithm (That is, ~24000 codewords are processed so this will take ~&frac12; hour). As before, the graphs will be printed into the graphs directory, and the variance vs. bit_error_probability will be saved in a csv file under the `stats` directory. Also, the `stats` directory will contain csv files for each variance level and each algorithm that hold the original codeword, its transmission values, and their resulting decoding.</span></p><p class="c0 c1"><span></span></p><p class="c0"><span>`python run.py -advanced`</span></p><h1 class="c0 c2"><a id="h.cmvbxggmbj7x"></a><span>Results &amp; Discussion</span></h1><p class="c0 c1"><span></span></p><p class="c0"><span>The result from running `python run.py 2000` generated the following plot on a logarithmic scale for </span><img src="images/image12.png"><span>&nbsp;vs. </span><img src="images/image13.png"><span>&nbsp;using 2000 codewords in each run of the algorithm at each variance level.</span></p><p class="c0 c1"><span></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 557.33px;"><img alt="" src="images/image18.png" style="width: 624.00px; height: 557.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c1"><span></span></p><p class="c0"><span>As you can see, both algorithms perform better when the variance in noise is smaller and the accuracy (ability to accurately decode a transmitted codeword) decreases when there is more noise added to the channel. The sum-prod seems to produce slightly better results at variance &frac14; and &frac12;</span><span>&nbsp;, although it is likely that the different transmissions between algorithms may have accounted for this slight difference. Generally speaking though, both algorithms perform similarly at all evaluated levels of variance in channel noise.</span></p><p class="c0 c1"><span></span></p><p class="c0"><span>More tests were run using 1000 codewords, but at more levels of variance. &nbsp;The results produced a more logarithmic graph for each algorithm. This again proved the idea that the decoder works very well on channels with little noise, and performance decreases as channel noise increases.</span></p><h1 class="c0 c2"><a id="h.31n62e3ltyw5"></a><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 607.50px; height: 541.94px;"><img alt="" src="images/image17.png" style="width: 607.50px; height: 541.94px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></h1><p class="c0 c1"><span></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 617.50px; height: 551.15px;"><img alt="" src="images/image16.png" style="width: 617.50px; height: 551.15px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0 c1"><span></span></p><p class="c0"><span>The flat lines indicate 0 bit errors at these variance levels. They are flat and the same because we always add 1 bit error when computing the error so that when converting the probability to logarithmic domain, we do not get a domain error (i.e. if the error rate is 0).</span></p><h1 class="c0 c2"><a id="h.2h5vs57qtaeu"></a><span>Conclusion</span></h1><p class="c0 c1"><span></span></p><p class="c0"><span>The use of Sum-prod and Max-prod algorithms to decode Hamming codes transmitted over channels with noise can be very effective. As one would guess, the less noise, the better these algorithms perform. </span></p><p class="c0 c1"><span></span></p><p class="c0"><span>Using a graphical model like a factor graph to represent the Global function for membership in this code allows us perform inference on the original codewords based on the observations of the received codeword after transmission through a channel with noise. That is we are able to compute the posterior distribution of the hidden variables given the observed variables.</span></p></body></html>